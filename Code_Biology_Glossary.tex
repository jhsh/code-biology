\documentclass[12pt]{article}

\usepackage{url}
\usepackage[colorlinks]{hyperref}

\hypersetup{
 linkcolor=blue
}

\title{Code Biology: A glossary of terms and concepts}
\author{Marcello Barbieri, Joachim de Beule, Jan-Hendrik Hofmeyr}
\date{\today}



% ----------------------------------------------------------------
\begin{document}

\maketitle

\setcounter{secnumdepth}{0}
\tableofcontents

\newpage

\section{Abduction}
A category of logic introduced by Charles Peirce in addition to the Aristotelian categories of `induction' and `deduction'. It is the operation of `jumping to conclusions', or `getting a result from incomplete data'. 


\hypertarget{adaptor}{}
\section{Adaptor}
An object that provides a link between independent entities such as signs and meanings. An adaptor is the physical structure that implements the rule of a code. In the Morse code, for example, the adaptors are the neural circuits that make connections in the brain between letters of the alphabet and groups of dot and dashes. In protein synthesis, the adaptors are the transfers RNAs, the molecules that establish a correspondence (or a mapping) between codons (organic signs) and amino acids (organic meanings). 


\hypertarget{agent}{}
\section{Agent}
An Agent is a `\textit{maker}', more precisely an `\textit{artifact-maker}'. It is a system that makes `artifacts', where the term `artifact' means any object that cannot be formed spontaneously in Nature. A novel, a painting or a computer, for example, are cultural artifacts. Genes and proteins are molecular artifacts because they are manufactured by molecular machines. Making an artifact means assembling it from components. A novel is assembled from words, a protein is assembled from amino acids, a gene is assembled from nucleotides. But the components of an artifact do not self-assemble spontaneously and it is the `artifact-maker' that arranges them in a specific order. This order, in turn, must come from some other object and the job of the artifact-maker is to assemble the components of an object (for example a protein) in the order provided by another object (for example a gene). This means that an artifact-maker is a system that makes use of objects that stand for other objects (for example codons that stand for amino acids), and these are, by definition, signs. An artifact-maker, therefore, is necessarily a semiotic system because it creates an arbitrary link between the objects of two independent worlds that are referred to as signs and meanings.

\hypertarget{alignment}{}
\section{Alignment}
%% TO DO
 
\section{Anthroposemiosis }
It is the semiosis that takes place only in humans as distinct from `zoosemiosis', the form that is typical of animals, and to `biosemiosis', the form that exists in all living creatures. According to some claims, there is also a form of semiosis in the physical world which is referred to as `physiosemiosis' (or `pansemiosis'), but there is no scientific evidence in support of this view.


\hypertarget{apoptosis}{}
\section{Apoptosis}
It is a form of cell death that is not caused by accidental injuries or old age but by active \textit{cell suicide}. This is induced by the activation of specific suicide genes, and it is a universal mechanism of embryonic development, one that is used to shape virtually all organs of the body. The key point is that suicide genes exist in \textit{all} cells and the signalling molecules that switch them on and off are of many different types. This means that the recognition of a signalling molecule and the activation of the suicide genes are two independent processes. There simply is no necessary connection between them and the only realistic solution is that the link is established by the rules of an \textit{Apoptosis code}, i.e., a code that determines which signalling molecules switch on the apoptosis genes in which tissue.


\section{Arbitrariness}
In science, arbitrariness means \textit{independence from physical necessity}. Codes and conventions, for example, are arbitrary because their rules are not dictated by the laws of physics. In the case of the genetic code, it is has been proven beyond doubt that any codon can be associated with any amino acids, just as any group of dots and dashes can be associated with any letter of the alphabet. The genetic code, in short, is a real code because it does have the essential feature that defines any code: the \textit{arbitrariness of the coding rules}. It must be underlined, however, that this point has raised streams of objections, all claiming that arbitrariness is a myth because there are all sorts of regularities in the genetic code. In reality, a few simple cases are enough to deflate this argument. In the Morse code, for example, the most frequent letters of the alphabet are associated with the simplest combinations of dots and dashes, but nobody would dream to conclude that the Morse code is not made of arbitrary rules because of that regularity. In any language there are countless regularities, and yet arbitrariness exists even in the number and the type of letters that make up an alphabet. Regularities, in short, are perfectly compatible with arbitrariness. What they are \textit{not} compatible with is \textit{randomness}, but arbitrariness should not be confused with randomness.


\section{Artifacts}
Artifacts are all objects that cannot be formed spontaneously in Nature. For a long time it has been assumed that artifacts are necessarily cultural products, i.e., that only man can produce them. In reality genes and proteins too are artifacts because they are manufactured by molecular machines. Primordial nucleic acids and primordial polypeptides did appear spontaneously on the primitive Earth, but could not evolve into the first cells because spontaneous molecules do not have biological specificity. They evolved instead into primitive molecular machines, and it was these machines and their products that eventually gave origin to the first cells. The crucial point is that genes and proteins with the same sequence cannot be reproduced \textit{indefinitely} by spontaneous reactions. This can be achieved only by manufacturing processes that stick monomers together in the order provided by a template. Only this operation can guarantee biological specificity, and it was therefore the appearance of `molecular machines', i.e., of `artifact-makers', or `agents', that produced the specific molecular artifacts that we call genes and proteins. 


\section{Autonomy}
In biology it is a form of relative independence from the environment, and it is achieved when living systems acquire the ability to store materials and energy for later use. It is also achieved when they acquire a greater degree of complexity in the course of evolution, because this allows them to increase the number and the quality of their interactions with the outside world. 


\hypertarget{biolinguistics}{}
\section{Biolinguistics}
It is a research field dedicated to studying language as any other attribute of our species, and more specifically, as an organ of the mind/brain. In the editorial of the first issue of the journal \textit{Biolinguistics} (2007) the editors Cedric Boeckx and Kleanthes Grohmann stated that there is both a weak and a strong sense to the term `biolinguistics'. The weak sense refers to the fact that linguists are seriously engaged in ``discovering the properties of grammar, in effect carrying out the research program Chomsky initiated in \textit{Syntactic Structures} (1957)''. The strong sense refers to ``attempts to provide explicit answers to questions that require the combination of linguistic insights and insights from related disciplines (evolutionary biology, genetics, neurology, psychology, etc.).'' They underlined that Eric Lenneberg's book, \textit{Biological Foundations of Language} (1967), was an outstanding example of research in biolinguistics in the strong sense.


\section{Biosemiotics}
The synthesis of biology and semiotics that today we call `biosemiotics' was developed independently in two fields that lie at the opposite ends of academia. The first origin took place in molecular biology as a result of the discovery of the genetic code (the name ``\textit{molecular biosemiotics}'' was coined by Marcel Florkin in 1974 precisely to designate the study of semiosis at the molecular level). The second origin took place in the humanities and was masterminded by Thomas Sebeok in two distinct stages. In 1963, Sebeok extended semiosis from human culture to all animals and founded the new research field of `zoosemiotics' (Sebeok 1963). More than 20 years later, he made a second extension from animals to all living creatures and called it `biosemiotics' (Anderson et al. 1984, Sebeok and Umiker-Sebeok 1992, Sebeok 2001). These two `birthplaces' of biosemiotics have nurtured two different concepts of semiosis that still divide the field into two opposite schools. In biology, the existence of a real genetic code is proof enough that semiosis exists at the molecular level, and this implies that \textit{organic semiosis is defined by coding}. In the humanities, the dominant view is the Peircean concept that semiosis is always an interpretive process, and this implies that \textit{Peircean semiosis is defined by interpretation}. We have therefore two types of semiosis, one based on coding and one based on interpretation, and each of them represents phenomena that undoubtedly exist in Nature. There is ample evidence that animals are capable of interpreting the world, and this clearly means that \textit{Peircean} (or \textit{interpretive}) semiosis is a reality. But it is also evident that the rules of the genetic code do not depend on interpretation because they have been the same in all living creatures and in all environments ever since the origin of life. The division between the two schools of biosemiotics is precisely about this point. According to the `biological' school, the two types of semiosis are both present in Nature and represent two distinct evolutionary developments. According to the Peircean school, instead, interpretive semiosis is the only type that has existed on Earth ever since the origin of life. 


\section{Brain}
The nervous system is made of three types of neurons: (1)~the \textit{sensory neurons} transmit the electrical signals produced by the sense organs, (2)~the \textit{motor neurons} deliver electrical signals to the motor organs (muscles and glands), and (3)~the \textit{intermediate neurons} provide a bridge between them. In some cases the sensory neurons are directly connected to the motor neurons, thus forming a \textit{reflex arch}, a system that provides a quick stimulus-response reaction known as a \textit{reflex}. In a few cases, therefore, intermediate neurons can be dispensed with, but most animals use them extensively, and what we observe in evolution is that brains increased their size primarily by increasing the number of the intermediate neurons. The evolution of the brain, in other words, has largely been the evolution of the intermediate brain, a process during which the intermediate neurons started processing the signals that they were transmitting, and that opened up a whole new world of possibilities. In practice, the \textit{processing} evolved in two great directions and produced two very different outcomes. One was the formation of neural networks that give origin to feedback systems and provide a sort of `automatic pilot' for any given physiological function. The other was the generation of feelings and instincts. The first processing was totally unconscious and was carried out by a component of the intermediate brain that can be referred to as the \textit{cybernetic brain}. The second processing was adopted by another major component of the intermediate brain that can be referred to as the \textit{instinctive brain}. The intermediate brain, in short, evolved from a primitive reflex-arch system and developed two distinct types of neural processing, one completely unconscious and the second controlled by instincts. 
 

\section{Brain as a modelling system}
The brain produces what we are used to call feelings, sensations, perceptions, mental images and so on, but it is convenient to have also a more general term that applies to all of them. Here we follow the convention that all products of brain processing can be referred to as brain models. The brain, in other words, uses the signals from the sense organs to generate models of the world. A visual image, for example is a model of the information delivered by the retina, and a feeling of hunger is a model obtained by processing the signals sent by the sense detectors of the digestive apparatus. The brain can be described in this way as a modelling system, a concept that has been popularized by Thomas Sebeok and has acquired an increasing importance in semiotics (Sebeok and Danesi, 2000). The term was actually coined by Juri Lotman, who described language as the `primary modelling system' of our species (Lotman, 1991), but Sebeok underlined that language evolved from animal systems, and should be regarded as a secondary, if not a tertiary, modelling system. The distinction between primary, secondary and tertiary modelling systems has become a matter of some controversy, so it is important to be clear about it. Here we use those terms to indicate the modelling systems that appeared at three different stages of evolution and were the results of three different types of brain processing. 


\section{Brain's first modelling system}
This is the system that appeared when the primitive brain managed to produce feelings and sensations. These entities can be divided into two major classes because the sense organs deliver information either about the outside world or about the interior of the body. The first modelling system consists therefore of two types of models, one that represents the environment and one that carries information about the body. Jakob von Uexküll (1909) called these two worlds \textit{Umwelt} and \textit{Innenwelt}, names that express very well the idea that every animal lives in two distinct subjective universes. We can say therefore that \textit{Innenwelt} is the model of the internal body built by the \textit{instinctive} brain, and that \textit{Umwelt} is the model of the external world built by the \textit{cybernetic} brain of an animal. The brain as we know it, came into being when the primordial brain split into instinctive brain and cybernetic brain, and these started producing the feelings and sensations that make up the first modelling system of all triploblastic animals (vertebrates and invertebrates). 


\section{Brain's second modelling system}
Some animals (like snakes) stop chasing a prey when this disappears from sight, whereas others (like mammals) deduce that the prey has temporarily been hidden by an obstacle and continue chasing it. Some can even learn to follow the footsteps of a prey, which reveals a still higher degree of abstraction. This ability to `interpret' the signals from the environment, is based on a new type of neural processing that makes use of memory, learning, the ability to `generalize' and in many cases also the faculty to `jump to conclusions' (abduction). All together these operations produce a model of the world that vastly expands the potentialities of the primitive brain and represents a \textit{second modelling system}, a system that appeared when part of the cybernetic brain became an `\textit{interpretive brain}'. 


\section{Brain's third modelling system}
The last major novelty in brain's history was the origin of language in our species, and that too required a new macroevolution, a new type of neural processing that went far beyond the reach of the interpretive brain because it was capable of coping with mental operations involving \textit{symbols}. This is why it is legitimate to say that language represents a \textit{third modelling system}. There have been, in conclusion, three major transitions in the evolution of the brain and each of them gave origin to a new type of neural processing that was, to all effects, a new modelling system. 

 
\section{Code}
A code is \textit{a set of rules that create a correspondence between two independent worlds}. The Morse code, for example, is a correspondence (or a \textit{mapping}) between the letters of the alphabet and groups of dots and dashes. The highway code is a correspondence between signals and driving behaviours. A language is a correspondence between words and objects. The genetic code is a correspondence between triplets of nucleotides, called codons, and amino acids. What is essential in all codes is that the coding rules are not dictated by the laws of physics. They are \textit{arbitrary} in the sense that they are \textit{independent from physical necessity} and this implies that they can be established only by natural or by cultural conventions. 


\section{Code Biology}
Code Biology is the study of all codes of life, from the genetic code to the codes of culture. The genetic code appeared on Earth at the origin of life, and the codes of culture arrived almost 4 billion years later, at the end of life's history. According to official (textbook) science, these are the only codes that exist in Nature, and if this were true we would have to conclude that codes are \textit{extraordinary exceptions} that appeared only at the beginning and at the end of evolution. In reality, various other organic codes (codes between organic molecules) have been discovered in the last 25 years, and it seems likely that many more will come to light in the future. The existence of many organic codes in Nature, however, is not only a new \textit{experimental fact}. It is one of those facts that have extraordinary theoretical implications. The first is that all great events of macroevolution were associated with the appearance of new organic codes, and this gives us a completely new description of the history of life. The second great implication is about the mechanisms of evolution. The discovery that there are \textit{two} fundamental molecular mechanisms at the basis of life, \textit{copying} and \textit{coding}, means that there are two distinct mechanisms of evolutionary change: \textit{evolution by natural selection}, based on copying, and \textit{evolution by natural conventions}, based on coding. The experimental discoveries and the theoretical implications of the organic codes make of Code Biology an entirely new field of research and an autonomous academic discipline, the real new frontier of biology. 


\section{Codemakers} (see Molecular machines) 


\section{Codepoiesis }
Before the origin of the genetic code, the ancestors of the first cells were engaged in the process of evolving coding rules and contained therefore a \textit{code generating system}. After the origin of the genetic code, however, the situation changed dramatically. No other modification in coding rules was tolerated and the system in question became a \textit{code conservation system}. Another part of the system, however, maintained the potential to evolve other coding rules and behaved as a new \textit{code generating}, or \textit{code exploring}, system. In the early Eukarya, for example, the cells had a \textit{code conservation part} for the genetic code, but also a \textit{code exploring part} for the splicing code. This tells us something important about life. The origin of the first cells was based on the ability of the ancestral systems to \textit{generate} the rules of the genetic code, and the subsequent evolution of the cells was based on two complementary processes: one was the \textit{generation} of new organic codes and the other was the \textit{conservation} of the existing ones. Taken together, these two processes are the two sides of a biological phenomenon that can be referred to as `\textit{codepoiesis}'. What is common to all living systems is either the generation or the conservation, or both, of organic codes, and this gives us an entirely new definition of the cell that can be expressed in this way: ``\textit{the cell is a codepoietic system, i.e., a system that is capable of creating and conserving its own codes}''. This definition accounts for the two most important events of evolution. [1] The ability to create coding rules accounts for the origin of the genetic code and of all the other codes that followed. [2] The ability of the cell to conserve its own codes accounts for the fact that \textit{the organic codes are the great invariants of life}, the entities that are conserved while everything else is changing. 


\section{Coding} (see Copying and coding)


\section{Coding semiosis }
Semiosis requires the existence of four distinct entities---signs, meanings, code and codemaker---because signs cannot exists without meanings and the relationship between them is necessarily based on the rules of a code, which in turn implies an agent that produces them, i.e., a codemaker. Semiosis, in other words, is necessarily based on coding, but this does not mean that it is based \textit{exclusively} on coding. In the course of evolution, some animals have acquired the ability to interpret the world, and interpretation is certainly a form of semiosis, since it makes use of signs and meanings, but it is not based on coding alone because it also requires memory, learning, mental representations and probably some form of `abduction'. For the first three thousand million years of evolution, the Earth has been inhabited exclusively by single cells, and these are capable of coding and decoding the world but do not make representations and therefore cannot interpret them. In order to distinguish the semiosis of single cells from that of animals it is convenient to use terms that qualify them and to this purpose the two types are referred to as \textit{coding semiosis} and \textit{interpretive semiosis}.


\section{Computable entities} (see Nominable entities)

\hypertarget{convention}{}
\section{Convention}

A convention or conventional code is a code that is shared among two
or more agents. \hyperlink{coordination}{Coordination} always relies
on a convention. The highway code is an example of a convention that
allows car drivers to coordinate their driving behavior. Language is
also a convention and is the result of coordination between
humans. Conventions can be dictated by a central authority (e.g. the
government), but the vast majority of conventions are the result of
\hyperlink{conventionalization}{conventionalization} processes. In
particular, the conventions that characterize the Major Transitions
are the result of \hyperlink{natural_conventionalization}{evolution by
  natural conventionalization}.

\hypertarget{conventionalization}{}
\section{Conventionalization}

Conventionalization is any process in which a
\hyperlink{convention}{convention} or shared code is established
between several agents. One well studied mechanism of
conventionalization in \hyperlink{semiotic_dynamics}{semiotic
  dynamics} is the process in which individual agents adapt their
codes through individual \hyperlink{learning}{learning} in order to
\hyperlink{alignment}{align} and make
\hyperlink{coordination}{coordination} between them possible. However,
conventionalization does not necessarily require that individual
agents can learn or adapt their codes. It is also possible that codes
are hardwired into agents but that different codes come into existence
by differential reproduction, so that offspring of agents possibly
employ different codes and a variety of coding behaviors exists at the
population level. Those agents in the population that are better
aligned may then have a \hyperlink{selection}{selective advantage}
over other agents, so that learning takes place at the population
level (see also \hyperlink{natural_conventionalization}{natural
  conventionalization}).

\section{Conventional signs} (see \hyperlink{signs}{Signs}, \hyperlink{peirce_model_of_semiosis}{Peirce model of semiosis}, and \hyperlink{convention}{Conventions})

\hypertarget{coordination}{}
\section{Coordination}

Coordination occurs when the combined (coding) behavior of two or
more interacting \hyperlink{agent}{agents} gives rise to a global or
collective phenomenon. Coordination thus always involves two levels of
\hyperlink{organization}{organization}: a microscopic level of
interacting agents and a macroscopic level at which the effect of
their coordination can be observed. Furthermore, in order for
coordination to occur, the agents must conform to a
\hyperlink{convention}{convention} or shared code. For instance, a
collection of cells may coordinate into a multi-celled body provided
that each cell fulfills (conforms to) it's
\hyperlink{function}{function} in the whole, that is, differentiates
into the appropriate cell-type during development and responds
appropriately to signals coming from other cells during life-time
etc. As another example, car drivers may coordinate into a crash-free
flow of traffic provided that all drivers conform to some highway
code, e.g. drive on the left side of the road etc. Humans coordinate
through culture and \hyperlink{language}{language}. From an
evolutionary perspective, coordination may increase the combined
\hyperlink{fitness}{fitness} of the coordinating agents. The Major
Transitions are extreme forms of coordination to the degree that
novel, irreducible agencies arise at the macroscopic level. In this
case the degree to which agencies at the microscopic level must
conform to the convention that supports coordination is such that they
sometimes even have to commit suicide (see \hyperlink{apoptosis}{apoptosis}).

\hypertarget{copying_and_coding}{}
\section{Copying and Coding}
Copying and coding are mechanisms that work at two distinct levels in every living system. Copying operates at the individual level of the molecules and coding at the collective level of the whole system. None of them is reducible to the other because they are \textit{complementary} mechanisms. They evolved in parallel in the history of life just as individual words and rules of grammar evolved in parallel in the history of language. There are, furthermore, other two important differences between copying and coding. One is the fact that copying produces either exact copies or slightly different versions of the copied molecules, which means that natural selection produces new objects by gradually modifying preexisting ones. Natural selection, in other words, creates only \textit{relative} novelties, not absolute ones. In the case of coding, instead, the situation is totally different. The rules of a code are not dictated by physical necessity, and this means that they can establish relationships that have never existed before in the Universe. Natural conventions, in short, have the potential to create \textit{absolute} novelties. Another difference between copying and coding is that they involve two different entities. A variation in the copying of a gene changes the linear sequence, i.e., the \textit{information} of that gene. A variation in a coding rule changes instead the \textit{meaning} of that rule. The great difference that exists between copying and coding, and therefore between natural selection and natural conventions, comes from the difference that exists between `information' and `meaning'. There are, in short, three major differences between copying and coding: (1) copying acts on individual molecules whereas coding acts at the collective level, (2) copying modifies existing objects whereas coding brings new objects into existence, and (3) copying is about biological information whereas coding is about biological meaning. 


\section{Copying semiosis}
In protein synthesis, a sequence of nucleotides is used to produce a sequence of amino acids according to the rules of the genetic code. In that case, there is no necessary connections between the components of the two molecules and the codons of nucleotides are used as \textit{conventional} organic signs, i.e., as organic\textit{ symbols}. A sequence of nucleotides, however, can also be used by a copymaker (a \textit{polymerase}) to produce a complementary copy of itself, and in that case the relationship between the two sequences is no longer established by \hyperlink{adaptor}{adaptors} but by direct physical interactions between complementary regions. These interactions, however, occur between very small regions of the molecules, and that means that the first sequence provides only a limited number of physical determinants for the second. The first sequence, in other words, does have a physical relationship with the second, but such relationship is undetermined and represents therefore only a `cue', i.e., a \textit{natural sign}, for the second. This means that the distinction between natural and conventional signs exists also at the molecular level, and represents in fact a divide between two very different types of processes. Sequences of nucleotides are used as conventional signs in coding and as natural signs in copying. Molecular coding, in short, is a form of \textit{coding semiosis} whereas molecular copying is a process of \textit{copying semiosis}. The translation of genes into proteins, in other words, is based on coding semiosis whereas the replication and the transcription of genes are based on copying semiosis.


\section{Copymakers} (see Molecular machines) 
 

\section{Cultural Semiosis} (see Popper's Three Worlds and …)


\section{Cybernetic brain} (see Brain)


\hypertarget{essential_parameters}{}
\section{Essential parameters}
The notion of essential parameters captures the essence of the
capacity of systems, such as \hyperlink{agent}{agents}, to persist
against the natural tendency of things to dissolve as a result of
disturbances from the environment. It was introduced by William Ross
Ashby in 1954 as follows: ``that a subsystem should keep its
integrity, that is not to disintegrate but remain as a subsystem,
certain parameters must remain within certain `physiological'
limits. What these parameters are, and what the limits, are fixed when
we have named the subsystem we are working with.'' For example, all
earthly systems, including the earth itself, have temperature among
their essential parameters, because if the surrounding temperature
rises above a certain limit no such system will persist. The notion of
essential parameters can be used to explain the Major Transition in
evolution as a result of
\hyperlink{natural_conventionalization}{evolution by natural
  conventionalization}.


\section{First modelling system} (see Brain's first modelling system)

\hypertarget{fitness}{}
\section{Fitness}

\hypertarget{function}{}
\section{Function}
%% TO DO

\section{Icons, Indexes and Symbols}
Peirce described three major types of signs, and called them \textit{icons}, \textit{indexes} and \textit{symbols}.
\begin{enumerate}
\item A sign is an \textit{icon} when it is associated with an object because of a \textit{similarity} between them. All trees, for example, are different, and yet they also have something in common and it is this common pattern that allows us to recognize any new tree that we happen to encounter for the first time. Icons, in other words, lead to pattern recognition and are the basic tools of perception. 

\item A sign is an \textit{index} when it is associated with an object because a \textit{physical link} is established between them. We learn to recognize a new cloud from previous clouds, and a new outbreak of rain from previous outbreaks, but we also learn that there is often a correlation between clouds and rain, and we end up with the conclusion that a black cloud is an index of rain. In the same way, the smell of smoke is an index of fire, footprints are indexes of preceding animals, and so on. Indexes, in short, are the basic tools of learning, because they allow animals to infer the existence of something from a few physical traces of it.

\item A sign is a \textit{symbol} when it is associated with an object because an \textit{arbitrary link} is established between them. There is no similarity and no physical link between a flag and a country, for example, or between a name and an object, and a relationship between them can exist only if it is the result of a convention. Symbols allow us to make arbitrary associations and build mental images of future events (projects), of abstract things (numbers), and even of non-existing things (unicorns). 
\end{enumerate}


\section{Information} (see Organic information) 


\section{Innenwelt}
The model of the internal body built by the brain of an animal. It is a \textit{subjective} model, and any animal lives therefore in a body whose feelings and instincts are manufactured by its own brain.


\section{Instinctive brain} (see Brain)


\section{Intermediate brain} (see Brain)


\section{Interpretant} (see \hyperlink{peirce_model_of_semiosis}{Peirce model of semiosis})


\section{Interpretation}
Memory allows a system to compare a phenomenon with previous records of similar phenomena, and it is from such comparisons that a system can `learn' from experience. Memory is clearly a prerequisite for learning, but what does learning achieve? What is the point of storing mental representations and comparing them? So far, the best answer to this problem is probably the idea that memories and learning allows animals to \textit{interpret} the world. Interpretation, on the other hand, is a form of semiosis---because it is based on signs---but it is a new form because it also requires memory and learning. What is interpreted, furthermore, is not the world but \textit{representations} of the world, and only multicellular systems can build them. Single cells decode the signals from the environment but do not have the physical means to build internal representations of them and therefore cannot interpret them. They are sensitive to light, but do not `see'; they react to sounds but do not `hear'; they detect hormones but do not `smell' and do not `taste' them. It takes many cells which have undertaken specific processes of differentiation to allow a system to see, hear, smell and taste, so it is only multicellular creatures that have these experiences. The evolution from single cells to animals was a true macroevolution because it created absolute novelties such as feelings and instincts. Later on, another macroevolution gave to many animals the ability to \textit{interpret} the world, and we can actually prove that this ability evolved in stages. The origin of interpretation provided animals with a new means of obtaining information about the world---a \textit{second modelling system}---and gave origin to a new type of semiosis that can be referred to as \textit{interpretive}, or \textit{Peircean}, semiosis. 


\section{Interpreter} (see \hyperlink{peirce_model_of_semiosis}{Peirce model of semiosis})


\section{Interpretive semiosis} (see Coding semiosis and Biosemiotics)

\hypertarget{language}{}
\section{Language}
%% TO DO

\hypertarget{learning}{}
\section{Learning}
%% TO DO

\section{Macroevolution} 
There is a close association between the great events of macroevolution and the appearance of organic codes. The origin of the genetic code, for example, gave origin to \textit{biological specificity}, the most fundamental of life's properties. It was the origin of protein-based life, i.e. of life-as-we-know-it. The data from molecular biology have revealed that all known cells belong to three primary kingdoms, or domains, that have been referred to as Archaea, Bacteria and Eucarya (Woese, 1987, 2000). These cells have three distinct signalling systems, and this does suggest that each domain arose by the combination of the universal genetic code with three distinct signal-transduction codes. Another great innovation was brought about by the codes of splicing, because splicing requires a \textit{separation in time} between transcription and translation and that was a precondition for their \textit{separation in space}, i.e. for the \textit{origin of the nucleus}. Many other eukaryotic innovations were brought into existence by organic codes. The cytoskeleton codes, for example, allowed the cells to build their own scaffoldings, to change their own shapes and to perform their own movements, including those of mitosis and meiosis. The histone code provided the rules of chromatin regulation, the adhesion codes allowed the cells to aggregate in multicellular groups and the codes of pattern made it possible to create systems capable of embryonic development. The major events in the history of life, in short, went hand in hand with the appearance of new organic codes, and this suggests a deep link between codes and macroevolution. More precisely, it suggests that \textit{the great events of macroevolution were made possible by the appearance of new organic codes}. 


\section{Meaning} (see organic meaning)


\section{Modelling systems} (see Brain as a modelling system)
 

\section{Molecular machines}
The simplest molecular machines that could appear spontaneously on the primitive Earth were \textit{bondmakers}, molecules that could stick monomers together at random and produce polymers, such as polypeptides, polynucleotides and polysaccharides. Some bondmakers, furthermore, acquired the ability to join monomers together no longer at random, but in the order provided by a template. Those bondmakers started making copies of other molecules and became \textit{copymakers}. The appearance of bondmakers at an advanced stage of chemical evolution led therefore to a steady increase in the number of polymers, and eventually to the appearance of the first copymakers, the first molecular machines that started populating the Earth with copied molecules and gave origin to genes. The origin of proteins, on the other hand, was a much more complex affair, because proteins cannot be copied and yet the information to make them must come from molecules that can be copied, i.e., from nucleic acids. The problem is that there is no necessary link between nucleic acids and amino acids, and only the rules of a code could provide a bridge between them, which means that the evolution of protein synthesis had to go hand in hand with the evolution of the genetic code. Proteins arose therefore from the integration of two different processes, and the final machine that produced them was a \textit{code-and-template-dependent-peptide-maker}, or, more simply, a \textit{codemaker}. Life, in short, arose by two distinct mechanisms, copying and coding, and each of them was brought into existence by molecular machines. Copymakers generated genes and codemakers populated the Earth with proteins. They were, and still are, the fundamental `makers', the `agents', of life.


\hypertarget{natural_conventionalization}{}
\section{Natural conventionalization}

Under certain conditions, individual agents may benefit from
\hyperlink{coordination}{coordinating} into a higher level of
\hyperlink{organization}{organization}.  This is the case whenever
there is no conflict between the
\hyperlink{essential_parameters}{essential parameters} of the agents
and when in addition there is an overlap between them. In this case,
the total (combined) capacity of the agents to keep essential
parameters within physiological limits {\em per} essential variable is
increased, because the agents each contribute their {\em full}
capacity while they each only bring in {\em a fraction} of the
parameters that are shared. Since conflicts between the essential
parameters of different agents tend to be eliminated over the course
of evolution, for instance due to
\hyperlink{natural_selection}{natural selection}, the probability that
these conditions are fulfilled are beneficial tends to increase over
time, and will eventually become a certainty. If agents then are
capable of establishing a \hyperlink{convention}{conventional code}
through some process of
\hyperlink{conventionalization}{conventionalization}, coordination
will effectively occur.  This mechanism is called evolution by natural
conventionalization, and is responsible for the Major Transitions in
macro evolution. That coordination through a process of
conventionalization is responsible for the Major Transitions explains
why these transitions are always characterized by the appearance of
new codes (or new ways of transmitting information as Maynard-Smith an
Szathmary put it), for increased levels of inter-dependency, and why
they give rise to novel, higher level agencies consisting of many
(coordinated) lower level agencies.

\section{Natural conventions}
The discovery of the genetic code has proved that there are \textit{two} distinct molecular mechanisms at the basis of life, the \textit{copying} of genes and the \textit{coding} of proteins. The discovery of other organic codes, furthermore, allows us to generalize this conclusion because it proves that coding is not limited to protein synthesis. Copying and coding, in other words, are distinct molecular mechanisms that have operated in all living systems ever since the origin of life, and this suggests that there are two distinct mechanisms of evolution because an evolutionary mechanism is but the long term result of a fundamental molecular mechanism. More precisely, the fact that coding is not reducible to copying and the close relationship that exists between organic codes and macroevolution tell us that coding had indeed a fundamental role in the history of life. All of which means that there are two distinct types of evolutionary change: \textit{evolution by natural selection}, based on \textit{copying}, and \textit{evolution by natural conventions}, based on \textit{coding}.
 

\section{Natural selection}
The copying of a gene is the elementary act that leads to \textit{heredity}. When the process of copying is repeated indefinitely, however, another phenomenon comes into being. Copying mistakes become inevitable, and in a world of limited resources not all changes can be implemented, which means that a process of selection is bound to take place. Molecular copying, in short, leads to \textit{heredity}, and the indefinite repetition of molecular copying leads to \textit{natural selection}. That is how natural selection came into existence. Molecular copying started it and molecular copying has perpetuated it ever since. This means that natural selection would be the sole mechanism of evolution if copying were the sole molecular mechanism at the basis of life. (See also \hyperlink{selection}{selection}).


\section{Natural signs} (see \hyperlink{signs}{Signs} and \hyperlink{peirce_model_of_semiosis}{Peirce model of semiosis}) 


\section{Neural code}
The inputs to the nervous system come from the sense organs, but these organs arise from the basic histological tissues of the body, and these tissues (epithelial, connective, muscular and nervous tissues) are the same in all animals. All signals that are sent to the brain, in other words, come from organs produced by few universal tissues, and represent a limited number of universal inputs. But do we also have a limited number of universal outputs? The neural correlates of the sense organs (feelings and perceptions) can be recognized by the \textit{actions} that they produce, and there is ample evidence that all animals have the same basic \textit{instincts}. They all seem to experience hunger and thirst, fear and aggression, and they are all capable of reacting to stimuli such as light, sound and smells. The neural entities that correspond to the basic histological tissues, in other words, are associated with the basic animal instincts and these appear to be the same in all animals. What we observe, in conclusion, is a universal set of basic histological tissues on one side, a universal set of basic animal instincts on the other side, and a set of neural transformation processes in between. The most parsimonious explanation is that the neural processes in between are also a universal set of operations that represent the rules a neural code because there is no necessary link between tissues and feelings. It is plausible, in short, that there has been a (nearly) universal neural code at the origin of mind just as there has been as a (nearly) universal genetic code at the origin of life. 


\section{Neural semiosis} (see Popper's Three Worlds and …)	 


\section{Nominal entities}
Science is always expressed in words and we need to give \textit{names} to the objects and the processes that we observe in Nature. Names (or \textit{nominal entities}, to use a classical term) in general have nothing to do with the intrinsic features of the named objects, and are therefore mere labels that we attach to them. The deep divide that exists between `names' and `objects' has been at the centre of many controversies in the past, in particular of the celebrated medieval dispute over `nominal' \textit{versus} `real' entities. The relationship between names and objects is also a crucial issue in science, but here it has taken on a new form. Let us start by underlining that all names are sequences of characters (alphabetic, numerical or alpha-numerical) and that each sequence is unique. Names, in other words, have \textit{specificity}. In general, the specificity of a name has nothing to do with the characteristics of the named object, and in these cases we can truly say that names are mere labels. Science, however, has invented a new type of names where the sequence of characters does represent an order that is objectively present in the named objects, and in these cases we speak no longer of \textit{nominal} but of \textit{nominable} entities. 

\hypertarget{nominable_entity}{}
\section{Nominable entities}
The chemical formula of a molecule describes an objective sequence of atoms, and any atom can be described by the objective sequence of its quantum numbers. These sequences are true \textit{observables} because they describe features that we observe in Nature, and have an important characteristic in common. When atoms and molecules are formed spontaneously, their final sequences are completely determined by the interactions between their own components. In the case of a protein, instead, all its different amino acids interact by the same peptide bonds and a spontaneous assembly would produce a completely random order (which is incompatible with life). In this case, a specific sequence can be obtained only if the amino acids are put together by a molecular machine according to the order provided by a template that is \textit{external} to the protein itself. We need therefore to distinguish between two different types of observables. The sequence of quantum numbers in an atom, or the sequence of atoms in inorganic molecules, is determined \textit{from within}, by internal factors, whereas the sequence of amino acids in a protein is determined \textit{from without}, by external templates. In the first case the sequence is a \textit{physically computable} entity, in the sense that it is the result of physical forces, whereas in the second case it can only be described by `naming' its components, and is therefore a \textit{nominable} entity (this term should not be confused with the classical concept of \textit{nominal} entity, that applies to all names). A \textit{nominable} entity is not a label but an observable, and more precisely a \textit{non-computable} observable. All names, in conclusion, are specific sequences of characters, and in science can be divided into two great classes: labels and observables. The observables, in turn, can be divided into \textit{computable} entities and \textit{nominable} entities. Physics and chemistry deal exclusively with computable entities (physical quantities), whereas nominable entities (information and coding rules) exist only in living systems. 
 

\section{Observables}
The observables are entities (such as space, time, mass, force, energy, etc.) that science, and in particular physics, use to describe the world and the history of science has been systematically accompanied by the discovery of new observables. In Newton's physics, for example, the fundamental observables were time, space and mass, but then electromagnetism required the addition of electric charge and thermodynamics required the addition of temperature. In general, it is assumed that biology does not need new observables, but in reality the very opposite is true. Life is based on the copying of genes and on the coding of proteins and these processes require entities, like biological sequences and the rules of the genetic code, that have all the defining characteristics of \textit{new observables}. This is because the role of the observables is to describe the world and we simply cannot describe living systems without sequences and codes, just as we cannot describe physical systems without space, time, mass, temperature, etc. The only difference is that sequences and coding rules are \textit{non-computable observables}, but there is no doubt that observables they are (we do observe them in living systems) and that they are \textit{fundamental} observables (because we cannot describe living systems without them and because we cannot reduce them to anything else). 


\section{Organic Codes}
Are codes between organic molecules. Any organic code is a set of rules that establish a correspondence between two independent organic worlds by means of molecular structures, called \hyperlink{adaptor}{adaptors}, that perform two independent recognition processes at each step. In the genetic code, for example, the adaptors are the transfer-RNAs. The adaptors are required because the two worlds would no longer be independent if there were a necessary link between them, and a set of rules is required in order to guarantee the specificity of the correspondence. The adaptors are the key molecules in all organic codes. They are the molecular \textit{fingerprints} of the codes, and their presence in a biological process is a sure sign that that process is based on a code. In addition to the genetic code, the existence of many other organic codes has been reported so far. Among them: the \textit{sequence codes} (Trifonov, 1987, 1989; 1999), the \textit{adhesive code} (Redies and Takeichi, 1996; Shapiro and Colman, 1999), the \textit{splicing codes} (Barbieri, 1998, 2003; Pertea et al., 2007; Barash et al. 2010; Dihr et al., 2010), the \textit{signal transduction codes} (Barbieri, 1998, 2003), the \textit{sugar code} (Gabius, 2000, 2009), the \textit{histone code} (Strahl and Allis, 2000; Turner, 2000; 2002), the \textit{cytoskeleton codes} and the \textit{compartment codes} (Barbieri, 2003, 2008), the \textit{tubulin code} (Verhey and Gertig, 2007), a \textit{nuclear signalling code} (Maraldi, 2008), and the \textit{ubiquitin code} (Komander and Rape, 2012). 



\section{Organic information}
In genes and proteins, biological, or \textit{organic}, information has been defined as the specific sequence of their subunits. This definition however is not entirely satisfactory because it gives the impression that information is something that molecules have simply because they have a sequence. In reality, there are countless molecules which have a sequence but only in a few cases this becomes information. This happens only when a sequence provides a guideline to a copymaker in a process of copying. It is only an act of copying, in other words, that brings information into existence. This tells us that organic information is not just the specific sequence of a molecule, but \textit{the specific sequence produced by a copying process}. This definition underlines the fact that organic information is not a thing or a property, but the result of a process. It is, more precisely, an `operative' definition, because information is defined by the process that brings it into existence. It must also be underlined that organic information is neither a quantity (because a specific sequence cannot be measured), nor a quality (because it is an objective feature of all copied molecules), and belongs instead to a third class of objects that have been referred to as \hyperlink{nominable_entity}{\textit{nominable}} entities.


\section{Organic meaning}
The Morse code is a correspondence between the letters of the alphabet and groups of dots and dashes and in the same way the genetic code is a correspondence between groups of nucleotides and amino acids. Let us notice now that establishing a correspondence between, say, object 1 and object 2, is equivalent to saying that object 2 is the meaning of object 1. In the Morse code, for example, the rule that `dot-dash' corresponds to the letter `A', is equivalent to saying that letter `A' is the meaning of `dot-dash'. By the same token, the rule of the genetic code that a group of three nucleotides (a codon) corresponds to an amino acid is equivalent to saying that that amino acid is the \textit{organic meaning} of that codon. Anywhere there is a code, be it in the mental or in the organic world, there is meaning. We can say, therefore, that \textit{meaning is an entity which is related to another entity by a code or a convention}, and that organic meaning exists whenever an organic code exists. All we need to keep in mind is that \textit{meaning is a mental entity when the code is between mental objects, but it is an organic entity when the code is between organic molecules.} It must also be underlined that organic meaning---like organic information---is neither a quantity (because a coding rule cannot be measured), nor a quality (because the organic codes are objective features of life), and belongs instead to a third class of objects that have been referred to as \textit{nominable} entities.


\section{Organic semiosis} (see Signs and Biosemiotics)

\hypertarget{organization}{}
\section{Organization}
%% TO DO

\hypertarget{peirce_model_of_semiosis}{}
\section{Peirce model of semiosis}
According to the classical doctrine of semiosis developed by Aristotle, Augustine and Aquinas, signs are divided into two great classes that Augustine (389ad) called \textit{signa data} and \textit{signa naturalia}, a distinction that continues to these days with the terms \textit{conventional signs} and \textit{natural signs}. The conventional signs (or \textit{symbols}) are those where there is no \textit{physical} relationship between signs and objects (between a flag and a country, for example) and a link between them can only be established by arbitrary rules, i.e. by conventions. In natural signs, by contrast, a physical link is always present. Typical examples are the \textit{symptoms} that doctors use to diagnose illnesses (spots on the skin, a fever, a swollen area, etc.), as well as a variety of \textit{cues} (smoke as sign of fire, odours as signs of food, footprints as signs of organisms, etc.). In all these cases there is a physical relationship between the visible signs and the invisible entities that they point to, and yet the relationship is \textit{underdetermined}, so much so that it takes an act of interpretation to establish it. All this suggests that semiosis is based on interpretation in natural signs and on codes in conventional signs, as Aristotle, Augustine and Aquinas had indicated. On this point, however, Peirce broke with tradition, argued that codes too are interpretive processes and concluded that semiosis always requires interpretation. According to Peirce, in other words, the agent of semiosis is necessarily an \textit{interpreter}, and this is why Thomas Sebeok (2001) declared that ``\ldots \textit{there can be no semiosis without interpretability}''. This conclusion has become known as `the Peirce model of semiosis', a model that was expressed in formal terms in the treatise \textit{Semiotik/Semiotics} edited by Roland Posner, Klaus Robering and Thomas Sebeok (1997), in the following way: ``\textit{The necessary and sufficient condition for something to be a semiosis is that A interprets B as representing C, where A is the interpretant, B is an object and C is the meaning that A assigns to B}''.


\section{Peircean semiosis} (see Interpretation and Biosemiotics)


\section{Physical quantities} (see Sequences)


\section{Popper's Three Worlds} 
In the 1970s, Karl Popper argued that the unity of Nature is realized by the coexistence of three distinct domains, or `Worlds'. The first (World 1) is the domain of all material objects, physical and biological, i.e., atoms, galaxies and bodies. The second (World 2) is the domain of the mind, the subjective world of mental states, feelings, emotions and consciousness. The third (World 3) is the domain of all human artifacts and cultural products. The three worlds could hardly be more different, and yet they do have something in common. At the heart of all of them there are \textit{codes}. The genetic code and other organic codes in World 1, neural codes in World 2 and countless cultural codes in World 3. The three worlds of Popper correspond therefore to three major types of semiosis that are referred to as \textit{organic}, \textit{neural} and \textit{cultural} semiosis. 

\hypertarget{qualification}{}
\section{Qualification}
%% TO DO

\section{Quantities, qualities and nominables}
According to a long tradition, natural entities are divided into \textit{quantities} and \textit{qualities}. Quantities can be measured and are objective, whereas qualities are subjective and cannot be measured. In the case of the sequences of genes and proteins, however, this scheme breaks down. The `order' (or `information') of a sequence is not a quantity because it cannot be measured. But it is not a quality either, because the specific order of a sequence is a feature that we find in all organic molecules, and is therefore an objective feature of the world, not a subjective one. The same is true for the rule of a code. This too cannot be measured, so it is not a quantity, but it is not a quality either because the rules of the genetic code, for example, are the same for all observers in all living systems. A scheme based on quantities and qualities alone, in short, is not enough to describe the world. In addition to quantities (\textit{objective} and \textit{measurable}) and qualities (\textit{subjective} and \textit{unmeasurable}) we must recognize the existence in Nature of a third type of entities (\textit{objective} but \textit{unmeasurable}). The information carried by biological sequences (organic information) and the rules of the organic codes (organic meaning) belong precisely to this new type of entities, and we can also give them a suitable name. Since organic information and organic meaning can be described only by \textit{naming} their components, we can say that they are \textit{nominable entities}, or, more simply, `\textit{nominables}'.


\section{Representations} (see Interpretation)


\section{Second modelling system} (see Brain's second modelling system)

\hypertarget{selection}{}
\section{Selection}

According to the theory of \hyperlink{natural_selection}{natural
  selection}, evolution is the consequence of differential
copying. \hyperlink{copying_and_coding}{Copying} ensures that what has
already evolved continues to exist, whereas variation through {\em
  differential} copying leads to further evolution or
change. Selection then is the long-term result of copying, because as
more copies come into existence and resources are scarce, some copies
are bound to go extinct. This is the core of the Malthusian principle
of selection, which in turn forms the core of Darwin's theory of
natural selection. However, Malthusian selection is not the only
possible form of selection. As Eug\`ene Marais already remarked just
after Darwin published his theory on {\em The Origin of Species}, ``the
struggle for life is not merely the struggle against competing fellows
[but also] against opposing laws of matter which make for dissolution
and the hindrance of growth''. In other words selection is not so much
the preservation of the fit as it is the destruction of the unfit. More 
generally, selection is any process in which that which `works' is kept
or persists, and that which doesn't work is dismissed. The
determination of what `works', however, is a
\hyperlink{qualification}{qualification} process, and thus depends on
a measure of quality. Having increased access to scarce resources
compared to fellow copiers is but one possible measure, but others are
possible, such as having increased ability to persist against the
natural tendency of things to dissolve, or having increased capacity
to \hyperlink{coordinate}{coordinate}.

\section{Semantics}  (see \hyperlink{syntax_and_semantics}{Syntax and semantics})

\section{Semiosis}
Semiosis is \textit{the production of signs}, and semiotics is usually referred to as \textit{the study of signs} (from the Greek \textit{semeion}$=$sign) but these definitions are too restrictive because signs are always associated with other entities. A sign, to start with, is always linked to a \textit{meaning}, which implies that sign and meaning cannot be taken apart because they are the two sides of the same coin. Semiotics, therefore, is the study of signs and meanings together, and a system of signs, i.e., a \textit{semiotic system}, is always made of at least two distinct worlds: a world of entities that we call \textit{signs} and a world of entities that represent their \textit{meanings}. The link between sign and meaning, in turn, calls attention to a third entity, i.e., to their \textit{relationship}. A sign is a sign only when it stands for something that is \textit{other than itself}, and this otherness implies at least some degree of \textit{independence}. It means that there is no deterministic relationship between signs and meanings. A semiotic system, therefore, is not any combination of two distinct worlds. It is \textit{a combination of two worlds between which there is no necessary link}, and this implies that a bridge between the two worlds can be established only by \textit{conventional} rules, i.e., by the rules of a \textit{code}. This is what makes semiosis different from everything else: semiosis requires a system made of two independent worlds that are connected by the conventional rules of a code. A semiotic system, in other words, is necessarily made of at least three distinct entities: signs, meanings and code. Signs, meanings and code, however, do not come into existence of their own. There is always an `agent' that produces them, and that agent can be referred to as a \textit{codemaker}. In the case of culture, for example, the codemaker is the human brain; in the case of the cell, the codemaker is the ribonucleoprotein system that makes proteins according to the rules of the genetic code. We come in this way to a general conclusion that can be expressed in this way: \textit{a semiotic system consists of signs, meanings and code that are all produced by the same agent, i.e., by the same codemaker}.


\hypertarget{semiotic_dynamics}{}
\section{Semiotic dynamics}

Semiotic dynamics is the study of the self-organizing and evolutionary
dynamics that lead to the evolution of
\hyperlink{convention}{conventional} meaning and codes, including
\hyperlink{language}{language}. The term was first introduced in a
1999 paper by Luc Steels and Frederic Kaplan \cite{steels99ecal} which
reported on a study of the
\hyperlink{conventionalization}{conventionalization} dynamics leading
to the emergence of a shared lexicon in a group of autonomous
distributed agents situated and grounded in an open environment. Other
publications along this line of research include
\cite{DeBeule06Cross_Situational_learning,vylder06namingGameConvergenceJTB,devylder07:phd,wellens08:_flexib_word_meanin_in_embod_agent}. The
topic was later picked up by researchers in statistical physics (see,
among others,
\cite{baronchelli05sharpTransitionVocabulary,cattuto07:_semiot_dynam_and_collab_taggin,puglisi08culturalRoutePNAS,loreto12})
and is related to models of the evolution of cooperation and
conventions in the field of evolutionary game theory, specifically
signaling games and pre-play signaling
\cite{skyrms10:_signal_evolut_learn_infor,santos11:_co}. Some of the
main theoretical results are that conventionalization is bound to
occur in a population of agents if the agents are predisposed or at
least have an incentive to cooperate and there is {\em amplifying}
individual \hyperlink{learning}{learning}
\cite{vylder06namingGameConvergenceJTB,skyrms10:_signal_evolut_learn_infor}
or if there is learning at the population level
\cite{santos11:_co,beule12:_overc_traged_commun_hawk_dove_conven_codin};
and that the dynamics of conventionalization are like the dynamics of
phase transitions \cite{baronchelli05sharpTransitionVocabulary}. These
results are compatible with the hypothesis that the mechanism of
\hyperlink{natural_conventionalization}{ evolution by natural
  conventionalization} is responsible for the Major Transitions in macro evolution.


\section{Semiotic system} (see Semiosis)


\section{Sequences}
The sequences of genes and proteins are as essential to the description of life as the physical quantities, and this means that they have the same \textit{scientific `status'}. This conclusion, however, raises immediately a new problem, because there are two distinct groups of physical quantities: a small group of \textit{fundamental} quantities (space, time, mass, charge and temperature) and a much larger group of \textit{derived} quantities. This distinction applies to all objective entities, so we need to find out whether biological sequences belongs to the first or to the second group. Luckily, this problem has a straightforward solution because the sequences of genes and proteins have two very special characteristics. One is that \textit{a change in a single component of a biological sequence may produce a sequence which has entirely new properties}. This means that although a biological sequence can be said to have `components', it is at the same time a single indivisible whole. The second outstanding feature is that \textit{from the knowledge of $n$ elements of a biological sequence we cannot predict the element ($n+1$)}. This is equivalent to saying that a \textit{specific sequence cannot be described by anything simpler than itself}, so it cannot be a derived entity. Which means that biological sequences have the same scientific status as the fundamental quantities of physics, and are therefore a new type of fundamental observables. 


\hypertarget{signs}{}
\section{Signs}
Signs have been traditionally defined as ``something that stands for something else'', and in antiquity were divided into two great categories---\textit{conventional} signs and \textit{natural} signs---for two different reasons. One is because they derive either from nature (\textit{signa ex natura}) or from culture (\textit{signa ex cultura}). The other is because they are either symbols (\textit{signa symbolica}) or symptoms (\textit{signa symptomatica}). If we put together both characteristics, signs are defined in the following way: 
\begin{enumerate}
\item the conventional signs are \textit{signa symbolica ex cultura}, and
\item the natural signs are \textit{signa symptomatica ex natura}.

The discovery of the genetic code came as a bolt from the blue precisely because it revealed the existence of a \textit{third} category of signs that all thinkers of the past had not predicted: the existence of symbols that come from nature, not from culture. In addition to the two classical categories, therefore, we now have a third one:

\item the organic signs are \textit{signa symbolica ex natura}.
\end{enumerate}
This is the immense novelty of the genetic code. It brought to light a third type of semiosis that exists in the organic world and for this has been called organic semiosis.
 

\section{Symbols} (see Icons, Indexes and Symbols)

\hypertarget{syntax_and_semantics}{}
\section{Syntax and Semantics}

In \hyperlink{biolinguistics}{biolinguistics} syntax refers to the
study of the formal rules and principles that govern when sentences
are well formed, without taking into account their meaning. In logics
syntax refers to the formal rules that determine when logical
statements are well formed without taking into account their meaning
and, by extension, their truth-value.  In computer science syntax
refers to the rules that determine when statements in a computer
program are well formed without taking into account (the result of)
the actual computations denoted by the statements. Thus in general
syntax is defined as the study of the properties of well formed
statements in a symbol system without taking into account what the
meaning or semantics is of the symbols and statements.  There are some
subtleties in this definition however. Firstly, symbols do not exist
in or by themselves but only as the result of being qualified as such
by some system, just as codons only exist within the context of a
system consisting of a ribonucleoprotein and tRNA's. Secondly, the
definition does not specify what it means to be `well formed'. Again,
deciding whether a statement is `well formed' is an act of
\hyperlink{qualification}{qualification}. Since qualification is a
matter of semantics, these issues indicate that syntax and semantics
are in fact inseparable and intrinsically semiotic notions.

\section{Third modelling system} (see Brain's third modelling system)


\section{Umwelt}
The model of the external world built by the brain of an animal. It is a \textit{subjective} model, and any animal lives therefore in an environment whose sounds, images, smells and tastes are manufactured by its own brain.


\section{World 1, 2 and 3} (see Popper's Three Worlds)


\section{Zoosemiotics}
It is the study of semiotics processes in animals, a field that Thomas Sebeok started with a research paper in 1963 and to which he gave its present name in the book \textit{Perspectives in Zoosemiotics} (1972). 
 

\bibliography{jhsh,Code_Biology_Glossary}
\bibliographystyle{elsart-num}


\end{document}
% ----------------------------------------------------------------
